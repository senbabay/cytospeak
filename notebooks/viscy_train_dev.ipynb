{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1176c1",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c214670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import signal\n",
    "import argparse\n",
    "import yaml\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.plugins.environments import SLURMEnvironment\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from transformers.optimization import get_scheduler\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8575735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from viscy.data.triplet import TripletDataModule\n",
    "from viscy.transforms import ScaleIntensityRangePercentilesd, NormalizeSampled, Decollated\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, Any, Iterable, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1403e89",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39a024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Helper functions\n",
    "# -----------------------\n",
    "def as_int_or_str(v, default):\n",
    "    if v is None:\n",
    "        return default\n",
    "    if isinstance(v, int):\n",
    "        return v\n",
    "    if isinstance(v, str):\n",
    "        s = v.strip()\n",
    "        if s.isdigit():\n",
    "            return int(s)\n",
    "        return s  # e.g., \"auto\"\n",
    "    return v\n",
    "\n",
    "def to_int(v, default=None):\n",
    "    if v is None: return default\n",
    "    if isinstance(v, int): return v\n",
    "    if isinstance(v, str) and v.strip().isdigit(): return int(v)\n",
    "    return int(float(v))  # handles \"3.0\"\n",
    "\n",
    "def to_float(v, default=None):\n",
    "    if v is None: return default\n",
    "    if isinstance(v, (int, float)): return float(v)\n",
    "    return float(str(v).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9114a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", type=str, default=\"qwen_config_notebooks.yaml\")\n",
    "\n",
    "# mirror TrainingArguments you shared\n",
    "parser.add_argument(\"--epochs\", type=int, default=None)                     # num_train_epochs\n",
    "parser.add_argument(\"--lr\", type=float, default=None)                       # learning_rate\n",
    "parser.add_argument(\"--lr_scheduler_type\", type=str, default=None,          # lr_scheduler_type\n",
    "                    choices=[\"linear\",\"cosine\",\"cosine_with_restarts\",\"polynomial\",\"constant\",\"constant_with_warmup\"])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=None)                 # per_device_train_batch_size\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=None)            # per_device_eval_batch_size\n",
    "parser.add_argument(\"--grad_accum\", type=int, default=None)                 # gradient_accumulation_steps\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=None)\n",
    "parser.add_argument(\"--logging_fraction\", type=float, default=None)         # to derive logging_steps\n",
    "parser.add_argument(\"--eval_fraction\", type=float, default=None)            # to derive eval_steps\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=None)               # optional\n",
    "parser.add_argument(\"--warmup_ratio\", type=float, default=None)             # optional, if steps not given\n",
    "\n",
    "# runtime/system\n",
    "parser.add_argument(\"--model_id\", type=str, default=None)\n",
    "parser.add_argument(\"--dataset\", type=str, default=None)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=None)\n",
    "parser.add_argument(\"--precision\", type=str, default=None, choices=[\"bf16-mixed\",\"16-mixed\",\"32-true\"])\n",
    "parser.add_argument(\"--devices\", type=int, default=None)\n",
    "parser.add_argument(\"--strategy\", type=str, default=None)\n",
    "parser.add_argument(\"--seed\", type=int, default=None)\n",
    "parser.add_argument(\"--tf32\", action=\"store_true\")\n",
    "parser.add_argument(\"--no_tf32\", dest=\"tf32\", action=\"store_false\")\n",
    "parser.set_defaults(tf32=True)\n",
    "\n",
    "# Single or Multi GPU training\n",
    "parser.add_argument(\"--device_map\", type=str, default=None,\n",
    "                help='Hugging Face device_map (e.g., \"auto\"). Usually leave unset/None when using Lightning.')\n",
    "\n",
    "\n",
    "# logging / names / output\n",
    "parser.add_argument(\"--run_name\", type=str, default=None)\n",
    "parser.add_argument(\"--logging_dir\", type=str, default=None)\n",
    "parser.add_argument(\"--output_dir\", type=str, default=None)\n",
    "\n",
    "# GC controls (Trainer: gradient_checkpointing, kwargs)\n",
    "parser.add_argument(\"--gradient_checkpointing\", action=\"store_true\")\n",
    "parser.add_argument(\"--no_gradient_checkpointing\", dest=\"gradient_checkpointing\", action=\"store_false\")\n",
    "parser.set_defaults(gradient_checkpointing=True)\n",
    "parser.add_argument(\"--gc_use_reentrant\", action=\"store_true\")\n",
    "parser.add_argument(\"--gc_no_reentrant\", dest=\"gc_use_reentrant\", action=\"store_false\")\n",
    "parser.set_defaults(gc_use_reentrant=False)  # Qwen-friendly default\n",
    "\n",
    "# HF Hub\n",
    "parser.add_argument(\"--hub_model_id\", type=str, default=None)\n",
    "\n",
    "# ---- LoRA CLI overrides \n",
    "parser.add_argument(\"--lora_r\", type=int, default=None)\n",
    "parser.add_argument(\"--lora_alpha\", type=float, default=None)\n",
    "parser.add_argument(\"--lora_dropout\", type=float, default=None)\n",
    "parser.add_argument(\"--lora_bias\", type=str, default=None, choices=[\"none\",\"all\",\"lora_only\"])\n",
    "parser.add_argument(\"--lora_task_type\", type=str, default=None, help=\"e.g., CAUSAL_LM\")\n",
    "\n",
    "parser.add_argument(\"--lora_use_rslora\", dest=\"lora_use_rslora\", action=\"store_true\")\n",
    "parser.add_argument(\"--lora_no_use_rslora\", dest=\"lora_use_rslora\", action=\"store_false\")\n",
    "parser.set_defaults(lora_use_rslora=None)  # None = not provided on CLI\n",
    "\n",
    "parser.add_argument(\"--lora_target_modules\", type=str, default=None,\n",
    "                    help=\"Comma-separated list e.g. 'q_proj,k_proj,v_proj,o_proj,...'\")\n",
    "parser.add_argument(\"--lora_modules_to_save\", type=str, default=None,\n",
    "                    help=\"Comma-separated list e.g. 'lm_head,embed_tokens'\")\n",
    "\n",
    "parser.add_argument(\"--lora_extras_yaml\", type=str, default=None,\n",
    "                    help=\"Inline YAML/JSON dict of extra LoraConfig fields (e.g., rank_pattern)\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# Load YAML and merge\n",
    "with open(args.config, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "def pick(key, default=None):\n",
    "    return getattr(args, key) if getattr(args, key) is not None else config.get(key, default)\n",
    "\n",
    "\n",
    "# Map your provided defaults\n",
    "epochs          = to_int(pick(\"epochs\", 3))\n",
    "lr              = to_float(pick(\"lr\", 1e-4)) \n",
    "lr_scheduler    = pick(\"lr_scheduler_type\", \"linear\")\n",
    "batch_size      = to_int(pick(\"batch_size\", 1))\n",
    "eval_batch_size = to_int(pick(\"eval_batch_size\", 1))\n",
    "grad_accum      = to_int(pick(\"grad_accum\", 4))\n",
    "weight_decay    = pick(\"weight_decay\", 0.01)\n",
    "logging_frac    = to_float(pick(\"logging_fraction\", 0.10))\n",
    "eval_frac       = to_float(pick(\"eval_fraction\", 0.10))\n",
    "warmup_steps_cfg= pick(\"warmup_steps\", None)\n",
    "warmup_ratio    = pick(\"warmup_ratio\", None)  # if you decide to use ratio\n",
    "if warmup_steps_cfg is not None: warmup_steps_cfg = to_int(warmup_steps_cfg)\n",
    "if warmup_ratio is not None:     warmup_ratio     = to_float(warmup_ratio)\n",
    "\n",
    "model_id        = pick(\"model_id\")\n",
    "dataset_name    = pick(\"dataset\")\n",
    "num_workers     = to_int(pick(\"num_workers\", 4))\n",
    "precision       = pick(\"precision\", \"bf16-mixed\")\n",
    "# devices/strategy with normalization\n",
    "devices_raw     = pick(\"devices\", 1)           # may be int or \"auto\"\n",
    "strategy        = pick(\"strategy\", \"auto\")\n",
    "devices         = as_int_or_str(devices_raw, 1)\n",
    "device_map      = pick(\"device_map\", None)\n",
    "\n",
    "seed            = pick(\"seed\", 42)\n",
    "tf32            = args.tf32 if \"tf32\" in args else config.get(\"tf32\", True)\n",
    "\n",
    "run_name        = pick(\"run_name\", f\"dynacell-{lr}_lr-{epochs}_epochs-{lr_scheduler}_schedule-completions\")\n",
    "logging_dir     = pick(\"logging_dir\", f\"./logs/{run_name}\")\n",
    "output_dir      = pick(\"output_dir\", \"fine-tuned-model\")\n",
    "\n",
    "gradient_checkpointing = args.gradient_checkpointing if \"gradient_checkpointing\" in args else config.get(\"gradient_checkpointing\", True)\n",
    "gc_use_reentrant       = args.gc_use_reentrant if \"gc_use_reentrant\" in args else config.get(\"gc_use_reentrant\", False)\n",
    "\n",
    "hub_model_id    = pick(\"hub_model_id\", \"shenbaba/Qwen2.5-VLM-3B-dynacell\")\n",
    "\n",
    " # If Lightning is doing multi-GPU or non-auto strategy, force HF device_map=None\n",
    "multi_gpu = (isinstance(devices, int) and devices > 1)\n",
    "non_auto_devices = (isinstance(devices, str) and devices not in (None, \"auto\"))\n",
    "if multi_gpu or non_auto_devices or (strategy and strategy != \"auto\"):\n",
    "    device_map = None\n",
    "\n",
    "# --- LoRA config (YAML + CLI overrides via pick-like behavior) ---\n",
    "lora_from_yaml = config.get(\"lora\", {}) or {}\n",
    "_default_lora = {\n",
    "    \"r\": 32,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"use_rslora\": True,\n",
    "    \"target_modules\": [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\",\"mlp.0\",\"mlp.2\"],\n",
    "    \"modules_to_save\": [\"lm_head\",\"embed_tokens\"],\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "}\n",
    "\n",
    "def _split_csv(s):\n",
    "    return [x.strip() for x in s.split(\",\")] if s else None\n",
    "\n",
    "def pick_lora(field, default=None):\n",
    "    # CLI first\n",
    "    cli_val = getattr(args, f\"lora_{field}\", None)\n",
    "    if cli_val is not None:\n",
    "        return cli_val\n",
    "    # YAML next\n",
    "    if field in lora_from_yaml and lora_from_yaml[field] is not None:\n",
    "        return lora_from_yaml[field]\n",
    "    # Fallback\n",
    "    return _default_lora.get(field, default)\n",
    "\n",
    "lora_cfg = {\n",
    "    \"r\": to_int(pick_lora(\"r\")),\n",
    "    \"lora_alpha\": to_float(pick_lora(\"lora_alpha\")),\n",
    "    \"lora_dropout\": to_float(pick_lora(\"lora_dropout\")),\n",
    "    \"use_rslora\": pick_lora(\"use_rslora\"),\n",
    "    \"bias\": pick_lora(\"bias\"),\n",
    "    \"task_type\": pick_lora(\"task_type\"),\n",
    "    \"target_modules\": _split_csv(args.lora_target_modules)\n",
    "                        if args.lora_target_modules is not None\n",
    "                        else pick_lora(\"target_modules\"),\n",
    "    \"modules_to_save\": _split_csv(args.lora_modules_to_save)\n",
    "                        if args.lora_modules_to_save is not None\n",
    "                        else pick_lora(\"modules_to_save\"),\n",
    "}\n",
    "\n",
    "if args.lora_extras_yaml:\n",
    "    try:\n",
    "        extra = yaml.safe_load(args.lora_extras_yaml)\n",
    "        if isinstance(extra, dict):\n",
    "            lora_cfg.update(extra)\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee14e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV lookup\n",
    "\n",
    "def build_annotation_lookup(\n",
    "    csv_or_df: Any,\n",
    "    key_cols=(\"fov_name\",\"track_id\",\"t\",\"parent_id\"),\n",
    "    value_cols=(\"organelle\",\"predicted_cellstate\",\"predicted_infection\"),\n",
    "    caption_col: str | None = None,\n",
    "):\n",
    "    \"\"\"Return dict[(fov_name,track_id,t,parent_id)] -> {value_cols..., '__caption__': str}\"\"\"\n",
    "    df = pd.read_csv(csv_or_df) if isinstance(csv_or_df, (str, bytes)) else csv_or_df.copy()\n",
    "\n",
    "    # normalize dtypes for exact matching\n",
    "    df[\"fov_name\"] = df[\"fov_name\"].astype(str)\n",
    "    for c in (\"track_id\",\"t\",\"parent_id\"):\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "\n",
    "    def make_caption(row):\n",
    "        if caption_col and caption_col in row and pd.notna(row[caption_col]):\n",
    "            return str(row[caption_col]).strip()\n",
    "        org   = str(row.get(\"organelle\",\"unknown\")).strip()\n",
    "        phase = str(row.get(\"predicted_cellstate\",\"unknown\")).strip()\n",
    "        inf   = str(row.get(\"predicted_infection\",\"unknown\")).strip()\n",
    "        return f\"{org}; {phase}; {inf}\"\n",
    "\n",
    "    lookup: Dict[Tuple[Any,...], Dict[str,Any]] = {}\n",
    "    for _, row in df.iterrows():\n",
    "        key = tuple(row[c] for c in key_cols)\n",
    "        payload = {c: row[c] for c in value_cols if c in df.columns}\n",
    "        payload[\"__caption__\"] = make_caption(row)\n",
    "        lookup[key] = payload\n",
    "    return lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b5b1762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(seed, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1629ff",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eff1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/hpc/projects/intracellular_dashboard/organelle_dynamics/rerun/2025_07_24_A549_SEC61_TOMM20_G3BP1_ZIKV/4-phenotyping/train-test/2025_07_24_A549_SEC61_TOMM20_G3BP1_ZIKV.zarr\"\n",
    "tracks_path = \"/hpc/projects/intracellular_dashboard/organelle_dynamics/rerun/2025_07_24_A549_SEC61_TOMM20_G3BP1_ZIKV/1-preprocess/label-free/3-track/2025_07_24_A549_SEC61_TOMM20_G3BP1_ZIKV_cropped.zarr\"\n",
    "source_channel =  [\"Phase3D\", \"GFP EX488 EM525-45\", \"mCherry EX561 EM600-37\"]\n",
    "annot_path = \"/hpc/projects/intracellular_dashboard/organelle_dynamics/rerun/2025_07_24_A549_SEC61_TOMM20_G3BP1_ZIKV/4-phenotyping/cytospeak_annotations/2025_07_24_annotations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c768950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dict\n",
    "anno_lookup = build_annotation_lookup(annot_path, caption_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e584a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True) # change this back to use_fast=False if you run into issues\n",
    "# dataset = load_dataset(dataset_name)\n",
    "\n",
    "# disable rescaling (you handle scaling yourself)\n",
    "processor.image_processor.do_rescale = False\n",
    "\n",
    "# optionally also disable normalization if you already did it\n",
    "processor.image_processor.do_normalize = False\n",
    "\n",
    "# disable conversion to RGB\n",
    "processor.image_processor.do_convert_rgb = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181d0258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLImageProcessorFast {\n",
       "  \"crop_size\": null,\n",
       "  \"data_format\": \"channels_first\",\n",
       "  \"default_to_square\": true,\n",
       "  \"device\": null,\n",
       "  \"do_center_crop\": null,\n",
       "  \"do_convert_rgb\": false,\n",
       "  \"do_normalize\": false,\n",
       "  \"do_rescale\": false,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"Qwen2VLImageProcessorFast\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"input_data_format\": null,\n",
       "  \"max_pixels\": 12845056,\n",
       "  \"merge_size\": 2,\n",
       "  \"min_pixels\": 3136,\n",
       "  \"patch_size\": 14,\n",
       "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"return_tensors\": null,\n",
       "  \"size\": {\n",
       "    \"longest_edge\": 12845056,\n",
       "    \"shortest_edge\": 3136\n",
       "  },\n",
       "  \"temporal_patch_size\": 2\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c94a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: quick shrink for prototyping (same as before)\n",
    "# resize_image = lambda ex: {\"image\": ex[\"image\"].resize((ex[\"image\"].width // 4, ex[\"image\"].height // 4))}\n",
    "# train_dataset = dataset[\"train\"].map(resize_image)\n",
    "# val_dataset   = dataset[\"test\"].map(resize_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91c8c9",
   "metadata": {},
   "source": [
    "### VISCY Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f256f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8d97c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data module...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up data module...\")\n",
    "del dm\n",
    "dm = TripletDataModule(\n",
    "    data_path=data_path,\n",
    "    tracks_path=tracks_path,\n",
    "    source_channel=source_channel,\n",
    "    #batch_size=batch_size, # already set in the notebook\n",
    "    batch_size = 10, # testing\n",
    "    num_workers=num_workers, # already set in the notebook\n",
    "    z_range=(0,1),\n",
    "    initial_yx_patch_size=(256, 256),\n",
    "    final_yx_patch_size=(160, 160),\n",
    "    normalizations=[\n",
    "        NormalizeSampled(\n",
    "            keys=[\"Phase3D\"], level=\"fov_statistics\", subtrahend=\"mean\", divisor=\"std\"\n",
    "        ),\n",
    "        Decollated(\n",
    "            keys=source_channel\n",
    "        ),\n",
    "        ScaleIntensityRangePercentilesd(\n",
    "            keys=[\"GFP EX488 EM525-45\"], lower=50, upper=99, b_min=0.0, b_max=1.0\n",
    "        ),\n",
    "        ScaleIntensityRangePercentilesd(\n",
    "            keys=[\"mCherry EX561 EM600-37\"], lower=50, upper=99, b_min=0.0, b_max=1.0\n",
    "        ),\n",
    "\n",
    "    ],\n",
    "    return_negative=False\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c26f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader =dm.train_dataloader()\n",
    "dm.setup(\"predict\")\n",
    "test_loader = dm.predict_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78580db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = batch['index']\n",
    "\n",
    "# build lookup set of (fov_name, track_id, t, parent_id)\n",
    "quadruplets = set(zip(\n",
    "    d[\"fov_name\"],\n",
    "    d[\"track_id\"].tolist(),\n",
    "    d[\"t\"].tolist(),\n",
    "    d[\"parent_id\"].tolist()\n",
    "))\n",
    "\n",
    "# filter DataFrame\n",
    "batch_annot = annot[annot.apply(\n",
    "    lambda r: (r[\"fov_name\"], r[\"track_id\"], r[\"t\"], r[\"parent_id\"]) in quadruplets,\n",
    "    axis=1\n",
    ")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6363e60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 160, 160])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['anchor'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6d4a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) images -> [B, 3, H, W]\n",
    "if imgs.ndim == 5 and imgs.shape[2] == 1:\n",
    "    imgs = imgs.squeeze(2)\n",
    "assert imgs.ndim == 4 and imgs.shape[1] == 3, f\"Expected [B,3,H,W], got {tuple(imgs.shape)}\"\n",
    "\n",
    "B = imgs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ea24781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) extract quadruplets\n",
    "fovs = [str(s) for s in idx[\"fov_name\"]]\n",
    "tids = idx[\"track_id\"].tolist() if torch.is_tensor(idx[\"track_id\"]) else list(idx[\"track_id\"])\n",
    "ts   = idx[\"t\"].tolist()        if torch.is_tensor(idx[\"t\"])        else list(idx[\"t\"])\n",
    "pids = idx[\"parent_id\"].tolist()if torch.is_tensor(idx[\"parent_id\"]) else list(idx[\"parent_id\"])\n",
    "\n",
    "texts: List[str] = []\n",
    "images_for_qwen: List[List[torch.Tensor]] = []\n",
    "targets: List[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5631bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollatingLoader:\n",
    "    \"\"\"Wrap an existing DataLoader and apply `collator` to each batch.\"\"\"\n",
    "    def __init__(self, base_loader, collator):\n",
    "        self.base_loader = base_loader\n",
    "        self.collator = collator\n",
    "\n",
    "    def __iter__(self):\n",
    "        for raw_batch in self.base_loader:\n",
    "            yield self.collator(raw_batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_loader)\n",
    "\n",
    "    # Handy passthroughs so PL can read dataset/sampler/batch_size, etc.\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self.base_loader.dataset\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return getattr(self.base_loader, \"batch_size\", None)\n",
    "\n",
    "    @property\n",
    "    def sampler(self):\n",
    "        return getattr(self.base_loader, \"sampler\", None)\n",
    "\n",
    "    @property\n",
    "    def batch_sampler(self):\n",
    "        return getattr(self.base_loader, \"batch_sampler\", None)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # delegate anything else to the base loader (e.g., drop_last, pin_memory, etc.)\n",
    "        return getattr(self.base_loader, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd9d86",
   "metadata": {},
   "source": [
    "### Computing loss over answers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b16f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Custom collator for multimodal Qwen input\n",
    "# -----------------------\n",
    "class QwenIndexAnchorCollator:\n",
    "    \"\"\"\n",
    "    Expects a batch:\n",
    "      batch = {\n",
    "        \"index\": {\n",
    "          \"fov_name\": list[str],   # len B\n",
    "          \"track_id\": Tensor[B],   # int\n",
    "          \"t\":        Tensor[B],   # int\n",
    "          \"parent_id\":Tensor[B],   # int\n",
    "          # other fields are ignored\n",
    "        },\n",
    "        \"anchor\": Tensor[B, 3, 1, H, W],  # images\n",
    "      }\n",
    "\n",
    "    Produces a dict ready for Qwen forward(**dict), with assistant-only labels.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        processor,\n",
    "        anno_lookup: Dict[Tuple[Any,...], Dict[str,Any]],\n",
    "        question: str = (\n",
    "                            \"You are given a fluorescence microscopy image.\\n\\n\"\n",
    "                            \"Task: classify three attributes.\\n\"\n",
    "                            \"Output format: exactly three words separated by single spaces, in this order: \"\n",
    "                            \"ORGANELLE PHASE INFECTION\\n\"\n",
    "                            \"Allowed vocabularies:\\n\"\n",
    "                            \"- ORGANELLE ∈ {ER, mitochondria, golgi, lysosome, nucleus, stress_granule}\\n\"\n",
    "                            \"- PHASE ∈ {interphase, mitotic}\\n\"\n",
    "                            \"- INFECTION ∈ {infected, uninfected}\\n\"\n",
    "                            \"Rules: no punctuation, no explanations, no quotes, no newlines. \"\n",
    "                            \"If uncertain, guess the most likely label from the allowed set.\"\n",
    "                        ),\n",
    "        pad_to_multiple_of: int | None = 8,\n",
    "        fail_on_missing: bool = True,  # True: error; False: skip unmatched rows\n",
    "    ):\n",
    "        self.processor = processor\n",
    "        self.anno_lookup = anno_lookup\n",
    "        self.question = question\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "        self.fail_on_missing = fail_on_missing\n",
    "\n",
    "        # cache tokenizer ids we use often\n",
    "        self.pad_id = getattr(self.processor.tokenizer, \"pad_token_id\", None)\n",
    "        self.eos_id = getattr(self.processor.tokenizer, \"eos_token_id\", None)\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_subsequence(seq: torch.Tensor, subseq: torch.Tensor) -> int | None:\n",
    "        n, m = len(seq), len(subseq)\n",
    "        if m == 0 or m > n:\n",
    "            return None\n",
    "        # naive scan is fine at typical batch sizes\n",
    "        for s in range(n - m + 1):\n",
    "            if torch.equal(seq[s:s+m], subseq):\n",
    "                return s\n",
    "        return None\n",
    "\n",
    "    def __call__(self, batch: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        idx = batch[\"index\"]\n",
    "        imgs = batch[\"anchor\"]  # [B, 3, 1, H, W]\n",
    "\n",
    "        # 1) images -> [B, 3, H, W]\n",
    "        if imgs.ndim == 5 and imgs.shape[2] == 1:\n",
    "            imgs = imgs.squeeze(2)\n",
    "        assert imgs.ndim == 4 and imgs.shape[1] == 3, f\"Expected [B,3,H,W], got {tuple(imgs.shape)}\"\n",
    "\n",
    "        B = imgs.shape[0]\n",
    "\n",
    "        # 2) extract quadruplets\n",
    "        fovs = [str(s) for s in idx[\"fov_name\"]]\n",
    "        tids = idx[\"track_id\"].tolist() if torch.is_tensor(idx[\"track_id\"]) else list(idx[\"track_id\"])\n",
    "        ts   = idx[\"t\"].tolist()        if torch.is_tensor(idx[\"t\"])        else list(idx[\"t\"])\n",
    "        pids = idx[\"parent_id\"].tolist()if torch.is_tensor(idx[\"parent_id\"]) else list(idx[\"parent_id\"])\n",
    "\n",
    "        texts: List[str] = []\n",
    "        images_for_qwen: List[List[torch.Tensor]] = []\n",
    "        targets: List[str] = []\n",
    "\n",
    "        # 3) per-sample pairing + message building\n",
    "        kept = 0\n",
    "        for i in range(B):\n",
    "            key = (str(fovs[i]), int(tids[i]), int(ts[i]), int(pids[i]))\n",
    "            row = self.anno_lookup.get(key)\n",
    "            if row is None:\n",
    "                if self.fail_on_missing:\n",
    "                    raise KeyError(f\"Missing annotation for quadruplet {key}\")\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            caption = str(row[\"__caption__\"]).strip()\n",
    "\n",
    "            # ensure per-image tensor is [3,H,W] (not HxWx3)\n",
    "            im = imgs[i]\n",
    "            if im.ndim == 3 and im.shape[0] != 3 and im.shape[-1] == 3:\n",
    "                im = im.permute(2,0,1)\n",
    "\n",
    "            # Qwen chat\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": self.question},\n",
    "                    {\"type\": \"image\"},\n",
    "                ]},\n",
    "                {\"role\": \"assistant\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": caption}\n",
    "                ]},\n",
    "            ]\n",
    "            text = self.processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "\n",
    "            texts.append(text.strip())\n",
    "            images_for_qwen.append([im])   # list-of-images per sample\n",
    "            targets.append(caption)\n",
    "            kept += 1\n",
    "\n",
    "        if kept == 0:\n",
    "            raise RuntimeError(\"After filtering/missing annotations, the batch is empty.\")\n",
    "\n",
    "        # 4) processor (tokenize + patchify)\n",
    "        out = self.processor(\n",
    "            text=texts,\n",
    "            images=images_for_qwen,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of\n",
    "        )\n",
    "\n",
    "        # 5) assistant-only labels\n",
    "        labels = out[\"input_ids\"].clone()\n",
    "\n",
    "        # tokenize targets once (batched)\n",
    "        tgt_tok = self.processor.tokenizer(\n",
    "            targets, add_special_tokens=False, padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        for i, input_ids in enumerate(out[\"input_ids\"]):\n",
    "            tgt_ids = tgt_tok[\"input_ids\"][i]\n",
    "            if self.pad_id is not None:\n",
    "                tgt_ids = tgt_ids[tgt_ids != self.pad_id]\n",
    "            start = self._find_subsequence(input_ids, tgt_ids)\n",
    "            if start is not None:\n",
    "                end = start + len(tgt_ids)\n",
    "                if self.eos_id is not None and end < len(input_ids) and input_ids[end].item() == self.eos_id:\n",
    "                    end += 1\n",
    "                labels[i, :start] = -100\n",
    "                labels[i, end:]   = -100\n",
    "            else:\n",
    "                labels[i, :] = -100  # safe fallback if not found\n",
    "\n",
    "        out[\"labels\"] = labels\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3990675",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = QwenIndexAnchorCollator(processor, anno_lookup, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a4c3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loaders for lightning\n",
    "train_loader_qwen = CollatingLoader(train_loader, collator)\n",
    "test_loader_qwen   = CollatingLoader(test_loader, collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10902e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps accounting to mirror your Trainer math\n",
    "\n",
    "#dataset_len = len(train_dataset)\n",
    "dataset_len = len(train_loader)\n",
    "\n",
    "steps_per_epoch = math.ceil(dataset_len / (batch_size * max(1, grad_accum)))\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "# Fractions → concrete steps\n",
    "logging_steps = max(1, int(total_steps * float(logging_frac)))\n",
    "eval_steps    = max(1, int(total_steps * float(eval_frac)))\n",
    "\n",
    "# Warmup resolution\n",
    "if warmup_steps_cfg is not None:\n",
    "    warmup_steps = int(warmup_steps_cfg)\n",
    "elif warmup_ratio is not None:\n",
    "    warmup_steps = int(total_steps * float(warmup_ratio))\n",
    "else:\n",
    "    warmup_steps = 0  # matches your commented-out default\n",
    "\n",
    "# testing\n",
    "epochs = 1\n",
    "del logging_steps\n",
    "logging_steps = 20\n",
    "del eval_steps\n",
    "eval_steps = 20\n",
    "grad_accum = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf69da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54dd735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # -----------------------\n",
    "# # Dataloaders\n",
    "# # -----------------------\n",
    "# # Pin + persistent workers improve performance on repeated small batches\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=collator,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "#     persistent_workers=True,\n",
    "#     drop_last=True,\n",
    "# )\n",
    "# val_loader = DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=eval_batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=collator,\n",
    "#     num_workers=max(1, num_workers // 2),\n",
    "#     pin_memory=True,\n",
    "#     persistent_workers=True,\n",
    "#     drop_last=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6192f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Logging & checkpoints\n",
    "# -----------------------\n",
    "# Logger & callbacks (TensorBoard + step-based eval/checkpointing)\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=logging_dir,\n",
    "    name=run_name,\n",
    "    default_hp_metric=False  # prevents Lightning adding HP metric noise\n",
    ")\n",
    "ckpt_cb = ModelCheckpoint(\n",
    "    dirpath=output_dir,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,               # keep best model only (like save_total_limit=1)\n",
    "    every_n_train_steps=eval_steps,  # save on same cadence as eval\n",
    "    filename=\"step{step}-valloss{val_loss:.4f}\",\n",
    "    auto_insert_metric_name=False,\n",
    "    save_last=True,\n",
    ")\n",
    "lr_cb = LearningRateMonitor(logging_interval=\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7841fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# LightningModule with LoRA-wrapped Qwen\n",
    "# -----------------------\n",
    "class QwenLoraModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    LightningModule wrapping:\n",
    "    - Qwen2.5-VL model with LoRA applied\n",
    "    - AdamW optimizer with HF scheduler\n",
    "    - Optional gradient checkpointing for VRAM savings\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id,\n",
    "        lr,\n",
    "        weight_decay,\n",
    "        lr_scheduler_type,\n",
    "        warmup_steps,\n",
    "        num_training_steps,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        adam_epsilon=1e-8,\n",
    "        gradient_checkpointing=True,\n",
    "        gc_use_reentrant=False,  # False avoids Qwen checkpointing bug\n",
    "        attn_implementation=\"eager\",\n",
    "        tf32=True,\n",
    "        lora_cfg=None,\n",
    "        device_map=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Save all hparams including lora_cfg (except num_training_steps which is large/dynamic)\n",
    "        self.save_hyperparameters(ignore=[\"num_training_steps\"])\n",
    "        self.num_training_steps = num_training_steps\n",
    "\n",
    "        # Enable TensorFloat32 for faster matmul on Ampere+ GPUs\n",
    "        if tf32:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "        # Processor handles both text & image preprocessing\n",
    "        self.processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "        # Load Qwen base model in bf16 for memory savings\n",
    "        base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=attn_implementation,\n",
    "            device_map=device_map,\n",
    "        )\n",
    "\n",
    "        # --- Robust gradient checkpointing across Transformers versions ---\n",
    "        if gradient_checkpointing:\n",
    "            enabled = False\n",
    "            try:\n",
    "                # Newer API accepts kwargs dict\n",
    "                base_model.gradient_checkpointing_enable(\n",
    "                    gradient_checkpointing_kwargs={\"use_reentrant\": gc_use_reentrant}\n",
    "                )\n",
    "                enabled = True\n",
    "            except TypeError:\n",
    "                pass\n",
    "            if not enabled:\n",
    "                try:\n",
    "                    # Older API: no kwargs\n",
    "                    base_model.gradient_checkpointing_enable()\n",
    "                    enabled = True\n",
    "                except TypeError:\n",
    "                    # Very old fallbacks\n",
    "                    if hasattr(base_model, \"enable_gradient_checkpointing\"):\n",
    "                        base_model.enable_gradient_checkpointing()\n",
    "                        enabled = True\n",
    "                    elif hasattr(base_model, \"set_gradient_checkpointing\"):\n",
    "                        base_model.set_gradient_checkpointing(True)\n",
    "                        enabled = True\n",
    "            if hasattr(base_model, \"enable_input_require_grads\"):\n",
    "                base_model.enable_input_require_grads()\n",
    "        \n",
    "        # --- LoRA configuration from YAML ---\n",
    "        if not isinstance(lora_cfg, dict) or len(lora_cfg) == 0:\n",
    "            raise ValueError(\n",
    "                \"LoRA configuration is missing. Please provide a 'lora:' section in qwen2vl_config.yaml.\"\n",
    "            )\n",
    "        lora_config = LoraConfig(**lora_cfg)\n",
    "\n",
    "        # Get LoRA model\n",
    "        self.model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "        # Training-friendly defaults\n",
    "        if hasattr(self.model, \"config\"):\n",
    "            self.model.config.use_cache = False\n",
    "            if getattr(self.model.config, \"pad_token_id\", None) is None:\n",
    "                self.model.config.pad_token_id = self.processor.tokenizer.eos_token_id\n",
    "\n",
    "    def forward(self, **batch):\n",
    "        return self.model(**batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self(**batch)\n",
    "        # Log training loss per step (no epoch avg to match HF behavior)\n",
    "        self.log(\"train_loss\", out.loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        return out.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(**batch)\n",
    "        # Log validation loss averaged over an epoch\n",
    "        self.log(\"val_loss\", out.loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return out.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Set up:\n",
    "        - AdamW optimizer with HF's beta/eps/weight decay settings\n",
    "        - LR scheduler from transformers.optimization.get_scheduler\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            betas=(self.hparams.adam_beta1, self.hparams.adam_beta2),\n",
    "            eps=self.hparams.adam_epsilon,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = get_scheduler(\n",
    "            name=self.hparams.lr_scheduler_type,\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.num_training_steps\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",  # step-based scheduler like HF\n",
    "                \"frequency\": 1,\n",
    "                \"name\": \"lr\"\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2638164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5de9124cae2495a80406fb5f5fc5110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Module\n",
    "module = QwenLoraModule(\n",
    "    model_id=model_id,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_scheduler_type=lr_scheduler,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    gc_use_reentrant=gc_use_reentrant,\n",
    "    attn_implementation=\"eager\",\n",
    "    tf32=tf32,\n",
    "    lora_cfg=lora_cfg,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e6d7a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/mydata/yasin.senbabaoglu/anaconda/25.3.1/x86_64/envs/qwen2vl/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /hpc/mydata/yasin.senbabaoglu/anaconda/25.3.1/x86_64 ...\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "del trainer\n",
    "\n",
    "# Trainer (Lightning: step-based val via val_check_interval)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=devices,\n",
    "    precision=precision,\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=logging_steps,\n",
    "    logger=logger,\n",
    "    callbacks=[ckpt_cb, lr_cb],\n",
    "    accumulate_grad_batches=grad_accum,\n",
    "    plugins=[SLURMEnvironment(requeue_signal=signal.SIGUSR1)],\n",
    "    strategy=strategy,\n",
    "    val_check_interval=eval_steps,  # \"eval_strategy=steps\"\n",
    "    limit_train_batches=200,        # exactly 100 batches per epoch\n",
    "    limit_val_batches=40, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79ac8b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/mydata/yasin.senbabaoglu/anaconda/25.3.1/x86_64/envs/qwen2vl/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:701: Checkpoint directory /hpc/mydata/yasin.senbabaoglu/projects/qwen_test/notebooks/fine-tuned-model exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/hpc/mydata/yasin.senbabaoglu/anaconda/25.3.1/x86_64/envs/qwen2vl/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type                 | Params | Mode\n",
      "------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 4.4 B  | eval\n",
      "------------------------------------------------------\n",
      "637 M     Trainable params\n",
      "3.8 B     Non-trainable params\n",
      "4.4 B     Total params\n",
      "17,569.022Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "2342      Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be59363ab19344299d4556af4adb4959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b422877653ca4be2956a0390a9a09dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/mydata/yasin.senbabaoglu/anaconda/25.3.1/x86_64/envs/qwen2vl/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:417: `ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['train_loss', 'epoch', 'step']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d470767cb1144fe8c8923a324d58d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d9ac350fcd413f912c55c0bfda1851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fd08bcee74449c9368279f90f73c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e9466ece6347bfaaaa131da0b91601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4948dda565748379b3fbfaea4fff28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff335dd7e0e4011aeca7a34b4403961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a90ce3cb2a4fffa728e9319313d3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b965e49cff4f3fa82f0fc248ddc420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c09f18f20aa45aea10e85eda6a8add4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70484451e46d4f9d8b1f2bccfed6c470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(module, train_loader_qwen, test_loader_qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a1b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
